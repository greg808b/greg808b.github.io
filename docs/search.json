[
  {
    "objectID": "posts/post_5/index.html",
    "href": "posts/post_5/index.html",
    "title": "Anomaly and Outlier Detection",
    "section": "",
    "text": "DBSCAN labels for scatter plot"
  },
  {
    "objectID": "posts/post_3/index.html",
    "href": "posts/post_3/index.html",
    "title": "SVM compared with RF",
    "section": "",
    "text": "New blog\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nimport matplotlib.pyplot as plt\n\nX_train = preprocess_pipeline.fit_transform(train_data)\ny_train = train_data[\"Survived\"]\nforest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\nforest_clf.fit(X_train, y_train)\nX_test = preprocess_pipeline.transform(test_data)\ny_pred = forest_clf.predict(X_test)\nforest_scores = cross_val_score(forest_clf, X_train, y_train, cv=10)\nforest_scores.mean()\n\nfrom sklearn.svm import SVC\n\nsvm_clf = SVC(gamma=\"auto\")\nsvm_scores = cross_val_score(svm_clf, X_train, y_train, cv=10)\nsvm_scores.mean()\n\n\nplt.figure(figsize=(8, 4))\nplt.plot([1]*10, svm_scores, \".\")\nplt.plot([2]*10, forest_scores, \".\")\nplt.boxplot([svm_scores, forest_scores], labels=(\"SVM\", \"Random Forest\"))\nplt.ylabel(\"Accuracy\")\nplt.show()\n\nC:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning:\n\n`sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value."
  },
  {
    "objectID": "posts/post_1/index.html",
    "href": "posts/post_1/index.html",
    "title": "Post#1",
    "section": "",
    "text": "This is post displays the ROC curve which is a typical approach to showing the precision recall relationship.\n\n\nC:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:1002: FutureWarning:\n\nThe default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n\n\n\n\nfrom pathlib import Path\n\nIMAGES_PATH = Path() / \"images\" / \"classification\"\nIMAGES_PATH.mkdir(parents=True, exist_ok=True)\n\ndef save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n    if tight_layout:\n        plt.tight_layout()\n    plt.savefig(path, format=fig_extension, dpi=resolution)\n\n\nidx_for_90_precision = (precisions &gt;= 0.90).argmax()\nthreshold_for_90_precision = thresholds[idx_for_90_precision]\n\n\nfrom sklearn.metrics import roc_curve\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches  # extra code – for the curved arrow\n\nfpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\n\n\nidx_for_threshold_at_90 = (thresholds &lt;= threshold_for_90_precision).argmax()\ntpr_90, fpr_90 = tpr[idx_for_threshold_at_90], fpr[idx_for_threshold_at_90]\n\nplt.figure(figsize=(6, 5))  # extra code – not needed, just formatting\nplt.plot(fpr, tpr, linewidth=2, label=\"ROC curve\")\nplt.plot([0, 1], [0, 1], 'k:', label=\"Random classifier's ROC curve\")\nplt.plot([fpr_90], [tpr_90], \"ko\", label=\"Threshold for 90% precision\")\n\n# extra code – just beautifies and saves Figure 3–7\nplt.gca().add_patch(patches.FancyArrowPatch(\n    (0.20, 0.89), (0.07, 0.70),\n    connectionstyle=\"arc3,rad=.4\",\n    arrowstyle=\"Simple, tail_width=1.5, head_width=8, head_length=10\",\n    color=\"#444444\"))\nplt.text(0.12, 0.71, \"Higher\\nThreshold\", color=\"#333333\")\nplt.xlabel('False Positive Rate (Fall-Out, FPR)')\nplt.ylabel('True Positive Rate (Recall)')\nplt.grid()\nplt.axis([0, 1, 0, 1])\nplt.legend(loc=\"lower right\", fontsize=13)\nsave_fig(\"roc_curve_plot\")\n\nplt.show()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Greg Brinson’s Blog",
    "section": "",
    "text": "Probabiltiy Theory and Random Variables\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2023\n\n\nGreg Brinson\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2023\n\n\nGreg Brinson\n\n\n\n\n\n\n  \n\n\n\n\nLinear and Non-Linear Regression\n\n\n\n\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2023\n\n\nGreg Brinson\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2023\n\n\nGreg Brinson\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly and Outlier Detection\n\n\n\n\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2023\n\n\nGreg Brinson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post_2/index.html",
    "href": "posts/post_2/index.html",
    "title": "Precision and Recall",
    "section": "",
    "text": "This blog shows the relationship between precision and recall in a Machine Learning context.\n\nfrom sklearn.datasets import fetch_openml, fetch_20newsgroups\n\nmnist = fetch_openml('mnist_784', as_frame=False)\nX, y = mnist.data, mnist.target\n\nC:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:1002: FutureWarning:\n\nThe default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    X,\n    y,\n    test_size=10000,\n    random_state=42,\n)\ny_train_5 = y_train == '5'  # True for all 5s, False for all other digits\ny_test_5 = y_test == '5'\n\n\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.linear_model import SGDClassifier\n\nsgd_clf = SGDClassifier(random_state=42)\nsgd_clf.fit(X_train, y_train_5)\ny_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\ncm = confusion_matrix(y_train_5, y_train_pred)\ncmd = ConfusionMatrixDisplay(cm)\ncmd.plot();\n\n\n\n\nHere you can see the numerical representation of the True Positives and True Negatives.\n\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import PrecisionRecallDisplay, PredictionErrorDisplay\nfrom sklearn.metrics import precision_recall_curve\n\n\nsgd_clf = SGDClassifier(random_state=42)\nsgd_clf.fit(X_train, y_train_5)\ny_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method=\"decision_function\")\nprecisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\nPrecisionRecallDisplay(precisions, recalls).plot();\n\n\n\n\nThis is the graphical representation of Recall vs Precision, or the ratio of correct positive predictions vs correct negative predictions."
  },
  {
    "objectID": "posts/post_4/index.html",
    "href": "posts/post_4/index.html",
    "title": "Post#4",
    "section": "",
    "text": "here we go this is good"
  }
]