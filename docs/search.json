[
  {
    "objectID": "posts/post_5/index.html",
    "href": "posts/post_5/index.html",
    "title": "Anomaly and Outlier Detection",
    "section": "",
    "text": "-DBSCAN labels for scatter plot\n-Clustering can be used for topic\nDB Scan has built in anomaly detection in data that does not fit close enough to the clusters found.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_blobs\n\n# Generate sample data with blobs\ndata, _ = make_blobs(n_samples=300, centers=4, random_state=40, cluster_std=1.0)\n\n# Apply DBSCAN\ndbscan = DBSCAN(eps=0.6, min_samples=5)\nlabels = dbscan.fit_predict(data)\n\n# Plot the data points and cluster assignments\nplt.figure(figsize=(8, 6))\n\nunique_labels = set(labels)\ncolors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n\nfor label, color in zip(unique_labels, colors):\n    if label == -1:\n        color = [0, 0, 0, 1]  # Black color for noise points\n\n    class_member_mask = (labels == label)\n    xy = data[class_member_mask]\n    plt.scatter(xy[:, 0], xy[:, 1], c=[color], edgecolors='k', s=50, linewidth=0.5, label=f'Cluster {label}')\n\nplt.title('DBSCAN Clustering')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/post_3/index.html",
    "href": "posts/post_3/index.html",
    "title": "Linear and Non-Linear Regression",
    "section": "",
    "text": "Here you can see a linear prediction based on random data with a linear trend. The model finds the most accurate linear expression based on the given data.\n\n\n\n\n\nWhen predicting trends the algorithm can have high-bias and underfit the data. This leads to it being too simple and not notice more applicable functions. The algorithm can also get high-variance or overfitting in which the model is too trained to the training data that it does not predict accurately on test data.\nRidge and Lasso Regression are regularization techniques to prevent overfitting and improve the performance of linear regression models by adding a penalty term.\nRidge Regression aims to minimize the sum of squared differences between the observed and predicted values. Lasso Regression uses the absolute value of the coefficients and uses a penalty term.\n\n\nCode\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\nX_train = preprocess_pipeline.fit_transform(train_data)\ny_train = train_data[\"Survived\"]\nforest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\nforest_clf.fit(X_train, y_train)\nX_test = preprocess_pipeline.transform(test_data)\ny_pred = forest_clf.predict(X_test)\nforest_scores = cross_val_score(forest_clf, X_train, y_train, cv=10)\nforest_scores.mean()\n\nfrom sklearn.svm import SVC\n\nsvm_clf = SVC(gamma=\"auto\")\nsvm_scores = cross_val_score(svm_clf, X_train, y_train, cv=10)\nsvm_scores.mean()\n\n\nplt.figure(figsize=(8, 4))\nplt.plot([1]*10, svm_scores, \".\")\nplt.plot([2]*10, forest_scores, \".\")\nplt.boxplot([svm_scores, forest_scores], labels=(\"SVM\", \"Random Forest\"))\nplt.ylabel(\"Accuracy\")\nplt.show()\n\n\n\n\n\nHere you can see the accuracy between SVM and Random Forest. Two very different supervised learning models. RandomForest does Non linear regression."
  },
  {
    "objectID": "posts/post_1/index.html",
    "href": "posts/post_1/index.html",
    "title": "Probabiltiy Theory and Random Variables",
    "section": "",
    "text": "-Histogram"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Greg Brinson’s Blog",
    "section": "",
    "text": "Anomaly and Outlier Detection\n\n\n\n\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2023\n\n\nGreg Brinson\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2023\n\n\nGreg Brinson\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2023\n\n\nGreg Brinson\n\n\n\n\n\n\n  \n\n\n\n\nLinear and Non-Linear Regression\n\n\n\n\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2023\n\n\nGreg Brinson\n\n\n\n\n\n\n  \n\n\n\n\nProbabiltiy Theory and Random Variables\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2023\n\n\nGreg Brinson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post_2/index.html",
    "href": "posts/post_2/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering is one way to perform unsupervised learning. It seeks to find patterns and structure within data, grouping data points that share characteristics or features. Common algorithms that perform this are K-Means and DBScan. Each with their own prowess depending on the distribution of data. DB Scan is great at getting unique shapes that are of similar density, but it is slow. K-Means is very fast but only can find circular shapes and does so at random.\nK-Means takes an initial value for the amount of clusters to look for and does its best to fit that. There is an algorithm to find the optimal amount of clusters based on the scoring of inertia (weighted distance from cluster center of each data point) from which the user can get an accurate set-up. Typically K-Means performs a few iterations to find the best scoring distribution of clusters that will have the most meaningful groupings.\nHere is an instance of DB Scan doing clustering:\n\n\nCode\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_moons\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nX, y = make_moons(n_samples=1000, noise=0.05, random_state=42)\ndbscan = DBSCAN(eps=0.05, min_samples=5)\ndbscan.fit(X)\n\ndef plot_dbscan(dbscan, X, size, show_xlabels=True, show_ylabels=True):\n    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n    core_mask[dbscan.core_sample_indices_] = True\n    anomalies_mask = dbscan.labels_ == -1\n    non_core_mask = ~(core_mask | anomalies_mask)\n\n    cores = dbscan.components_\n    anomalies = X[anomalies_mask]\n    non_cores = X[non_core_mask]\n    \n    plt.scatter(cores[:, 0], cores[:, 1],\n                c=dbscan.labels_[core_mask], marker='o', s=size, cmap=\"Paired\")\n    plt.scatter(cores[:, 0], cores[:, 1], marker='*', s=20,\n                c=dbscan.labels_[core_mask])\n    plt.scatter(anomalies[:, 0], anomalies[:, 1],\n                c=\"r\", marker=\"x\", s=100)\n    plt.scatter(non_cores[:, 0], non_cores[:, 1],\n                c=dbscan.labels_[non_core_mask], marker=\".\")\n    if show_xlabels:\n        plt.xlabel(\"$x_1$\")\n    else:\n        plt.tick_params(labelbottom=False)\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", rotation=0)\n    else:\n        plt.tick_params(labelleft=False)\n    plt.title(f\"eps={dbscan.eps:.2f}, min_samples={dbscan.min_samples}\")\n    plt.grid()\n    plt.gca().set_axisbelow(True)\n\ndbscan2 = DBSCAN(eps=0.2)\ndbscan2.fit(X)\n\nplt.figure(figsize=(9, 3.2))\n\nplt.subplot(121)\nplot_dbscan(dbscan, X, size=100)\n\nplt.subplot(122)\nplot_dbscan(dbscan2, X, size=600, show_ylabels=False)\n\nplt.show()\n\n\n\n\n\nTaking these clusters the user can then put meaning to them and see if it is properly distributed. As you can see the DB Scan first has too small an epsilon and does not properly identify the two meaningful clusters.\nDB Scan makes its clusters based on distance between points so it does not require a preset number of clusters to look for.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_blobs\n\n# Generate sample data with blobs\ndata, _ = make_blobs(n_samples=300, centers=4, random_state=42, cluster_std=1.0)\n\n# Apply DBSCAN\ndbscan = DBSCAN(eps=0.5, min_samples=5)\nlabels = dbscan.fit_predict(data)\n\n# Plot the data points and cluster assignments\nplt.figure(figsize=(8, 6))\n\nunique_labels = set(labels)\ncolors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n\nfor label, color in zip(unique_labels, colors):\n    if label == -1:\n        color = [0, 0, 0, 1]  # Black color for noise points\n\n    class_member_mask = (labels == label)\n    xy = data[class_member_mask]\n    plt.scatter(xy[:, 0], xy[:, 1], c=[color], edgecolors='k', s=50, linewidth=0.5, label=f'Cluster {label}')\n\nplt.title('DBSCAN Clustering')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/post_4/index.html",
    "href": "posts/post_4/index.html",
    "title": "Classification",
    "section": "",
    "text": "Classification is assigning predefined labels to data based on its features. It facilitates the recognition of patterns and the generalization of insights from data. Typical algorithms that incorporate this style of Machine Learning are Logistic Regression, Decision Trees, and Support Vector Machines. One very applicable use in everyone’s daily life is the spam filter. Data is either classified as spam or ham, algorithms can be tuned with datasets to assign these classes to unlabeled data.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    X,\n    y,\n    test_size=10000,\n    random_state=42,\n)\ny_train_5 = y_train == '5'  # True for all 5s, False for all other digits\ny_test_5 = y_test == '5'\n\n\n\n\nCode\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.linear_model import SGDClassifier\n\nsgd_clf = SGDClassifier(random_state=42)\nsgd_clf.fit(X_train, y_train_5)\ny_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\ncm = confusion_matrix(y_train_5, y_train_pred)\ncmd = ConfusionMatrixDisplay(cm)\ncmd.plot();\n\n\n\n\n\nHere you can see the numerical representation of the True Positives and True Negatives.\n\n\nCode\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import PrecisionRecallDisplay, PredictionErrorDisplay\nfrom sklearn.metrics import precision_recall_curve\n\n\nsgd_clf = SGDClassifier(random_state=42)\nsgd_clf.fit(X_train, y_train_5)\ny_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method=\"decision_function\")\nprecisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\nPrecisionRecallDisplay(precisions, recalls).plot();\n\n\n\n\n\nThis graph shows the precision recall curve which is a similar plot to the ROC but is less dependent on similar class distribution in the data.\n\n\nCode\nfrom pathlib import Path\n\nIMAGES_PATH = Path() / \"images\" / \"classification\"\nIMAGES_PATH.mkdir(parents=True, exist_ok=True)\n\ndef save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n    if tight_layout:\n        plt.tight_layout()\n    plt.savefig(path, format=fig_extension, dpi=resolution)\n\n\n\n\nCode\nidx_for_90_precision = (precisions &gt;= 0.90).argmax()\nthreshold_for_90_precision = thresholds[idx_for_90_precision]\n\n\n\n\nCode\nfrom sklearn.metrics import roc_curve\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches  # extra code – for the curved arrow\n\nfpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\n\n\n\n\nCode\nidx_for_threshold_at_90 = (thresholds &lt;= threshold_for_90_precision).argmax()\ntpr_90, fpr_90 = tpr[idx_for_threshold_at_90], fpr[idx_for_threshold_at_90]\n\nplt.figure(figsize=(6, 5))  # extra code – not needed, just formatting\nplt.plot(fpr, tpr, linewidth=2, label=\"ROC curve\")\nplt.plot([0, 1], [0, 1], 'k:', label=\"Random classifier's ROC curve\")\nplt.plot([fpr_90], [tpr_90], \"ko\", label=\"Threshold for 90% precision\")\n\n# extra code – just beautifies and saves Figure 3–7\nplt.gca().add_patch(patches.FancyArrowPatch(\n    (0.20, 0.89), (0.07, 0.70),\n    connectionstyle=\"arc3,rad=.4\",\n    arrowstyle=\"Simple, tail_width=1.5, head_width=8, head_length=10\",\n    color=\"#444444\"))\nplt.text(0.12, 0.71, \"Higher\\nThreshold\", color=\"#333333\")\nplt.xlabel('False Positive Rate (Fall-Out, FPR)')\nplt.ylabel('True Positive Rate (Recall)')\nplt.grid()\nplt.axis([0, 1, 0, 1])\nplt.legend(loc=\"lower right\", fontsize=13)\nsave_fig(\"roc_curve_plot\")\n\nplt.show()"
  }
]