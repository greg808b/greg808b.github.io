[
  {
    "objectID": "posts/post_5/index.html",
    "href": "posts/post_5/index.html",
    "title": "Anomaly and Outlier Detection",
    "section": "",
    "text": "DBSCAN labels for scatter plot"
  },
  {
    "objectID": "posts/post_3/index.html",
    "href": "posts/post_3/index.html",
    "title": "SVM compared with RF",
    "section": "",
    "text": "New blog\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nimport matplotlib.pyplot as plt\n\nX_train = preprocess_pipeline.fit_transform(train_data)\ny_train = train_data[\"Survived\"]\nforest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\nforest_clf.fit(X_train, y_train)\nX_test = preprocess_pipeline.transform(test_data)\ny_pred = forest_clf.predict(X_test)\nforest_scores = cross_val_score(forest_clf, X_train, y_train, cv=10)\nforest_scores.mean()\n\nfrom sklearn.svm import SVC\n\nsvm_clf = SVC(gamma=\"auto\")\nsvm_scores = cross_val_score(svm_clf, X_train, y_train, cv=10)\nsvm_scores.mean()\n\n\nplt.figure(figsize=(8, 4))\nplt.plot([1]*10, svm_scores, \".\")\nplt.plot([2]*10, forest_scores, \".\")\nplt.boxplot([svm_scores, forest_scores], labels=(\"SVM\", \"Random Forest\"))\nplt.ylabel(\"Accuracy\")\nplt.show()\n\nC:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning:\n\n`sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value."
  },
  {
    "objectID": "posts/post_1/index.html",
    "href": "posts/post_1/index.html",
    "title": "Probabiltiy Theory and Random Variables",
    "section": "",
    "text": "Histogram"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Greg Brinsonâ€™s Blog",
    "section": "",
    "text": "Probabiltiy Theory and Random Variables\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2023\n\n\nGreg Brinson\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2023\n\n\nGreg Brinson\n\n\n\n\n\n\n  \n\n\n\n\nLinear and Non-Linear Regression\n\n\n\n\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2023\n\n\nGreg Brinson\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2023\n\n\nGreg Brinson\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly and Outlier Detection\n\n\n\n\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2023\n\n\nGreg Brinson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post_2/index.html",
    "href": "posts/post_2/index.html",
    "title": "Precision and Recall",
    "section": "",
    "text": "This blog shows the relationship between precision and recall in a Machine Learning context.\n\nfrom sklearn.datasets import fetch_openml, fetch_20newsgroups\n\nmnist = fetch_openml('mnist_784', as_frame=False)\nX, y = mnist.data, mnist.target\n\nC:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:1002: FutureWarning:\n\nThe default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    X,\n    y,\n    test_size=10000,\n    random_state=42,\n)\ny_train_5 = y_train == '5'  # True for all 5s, False for all other digits\ny_test_5 = y_test == '5'\n\n\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.linear_model import SGDClassifier\n\nsgd_clf = SGDClassifier(random_state=42)\nsgd_clf.fit(X_train, y_train_5)\ny_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\ncm = confusion_matrix(y_train_5, y_train_pred)\ncmd = ConfusionMatrixDisplay(cm)\ncmd.plot();\n\n\n\n\nHere you can see the numerical representation of the True Positives and True Negatives.\n\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import PrecisionRecallDisplay, PredictionErrorDisplay\nfrom sklearn.metrics import precision_recall_curve\n\n\nsgd_clf = SGDClassifier(random_state=42)\nsgd_clf.fit(X_train, y_train_5)\ny_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method=\"decision_function\")\nprecisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\nPrecisionRecallDisplay(precisions, recalls).plot();\n\n\n\n\nThis is the graphical representation of Recall vs Precision, or the ratio of correct positive predictions vs correct negative predictions."
  },
  {
    "objectID": "posts/post_4/index.html",
    "href": "posts/post_4/index.html",
    "title": "Post#4",
    "section": "",
    "text": "here we go this is good"
  }
]