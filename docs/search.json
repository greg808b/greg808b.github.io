[
  {
    "objectID": "posts/post_5/index.html",
    "href": "posts/post_5/index.html",
    "title": "Anomaly and Outlier Detection",
    "section": "",
    "text": "Anomaly and Outlier detection is a crucial function for Machine Learning applications. There will always be data that skews the models predictions. Being able to automatically detect and remove them from the model’s consideration for predictions. The real world applications are for situations such as fraud detection, network security, healthcare monitoring, and environmental monitoring."
  },
  {
    "objectID": "posts/post_3/index.html",
    "href": "posts/post_3/index.html",
    "title": "Linear and Non-Linear Regression",
    "section": "",
    "text": "Here you can see a linear prediction based on random data with a linear trend. The model finds the most accurate linear expression based on the given data and graphs its results. Since we created a linear relationship it will have a high accuracy. In other data sets there will not always be a linear relationship to find, which will require non linear regression.\n\n\n\n\n\nWhen predicting trends the algorithm can have high-bias and underfit the data. This leads to it being too simple and not notice more applicable functions. The algorithm can also get high-variance or overfitting in which the model is too trained to the training data that it does not predict accurately on test data.\nRidge and Lasso Regression are regularization techniques to prevent over fitting and improve the performance of linear regression models by adding a penalty term.\nRidge Regression aims to minimize the sum of squared differences between the observed and predicted values. Lasso Regression uses the absolute value of the coefficients and uses a penalty term."
  },
  {
    "objectID": "posts/post_1/index.html",
    "href": "posts/post_1/index.html",
    "title": "Probabiltiy Theory and Random Variables",
    "section": "",
    "text": "Probability is a major part of the underlying pieces of Machine Learning. When preforming actual tasks, it is not immediately apparent the role that it plays. Machine learning is all about defining the probability that some data relates to a class. Two examples that show probability in action are the Probability Density Function (PDF) and the Kernel Density Estimation (KDE). A histogram is another useful probability plot that shows the actual value distributions of the data in a density form. Shown below is an example of a PDF using the skl iris dataset. It plots the density of sepal length and how present each data point was within the set. It is a jagged plot due to the small amount of data points available."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Greg Brinson’s Blog",
    "section": "",
    "text": "Anomaly and Outlier Detection\n\n\n\n\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2023\n\n\nGreg Brinson\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2023\n\n\nGreg Brinson\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2023\n\n\nGreg Brinson\n\n\n\n\n\n\n  \n\n\n\n\nLinear and Non-Linear Regression\n\n\n\n\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2023\n\n\nGreg Brinson\n\n\n\n\n\n\n  \n\n\n\n\nProbabiltiy Theory and Random Variables\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2023\n\n\nGreg Brinson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post_2/index.html",
    "href": "posts/post_2/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering is one way to perform unsupervised learning. It seeks to find patterns and structure within data, grouping data points that share characteristics or features. Common algorithms that perform this are K-Means and DBScan. Each with their own prowess depending on the distribution of data. DB Scan is great at getting unique shapes that are of similar density, but it is slow. K-Means is very fast but only can find circular shapes and does so at random.\nK-Means takes an initial value for the amount of clusters to look for and does its best to fit that. There is an algorithm to find the optimal amount of clusters based on the scoring of inertia (weighted distance from cluster center of each data point) from which the user can get an accurate set-up. Typically K-Means performs a few iterations to find the best scoring distribution of clusters that will have the most meaningful groupings."
  },
  {
    "objectID": "posts/post_4/index.html",
    "href": "posts/post_4/index.html",
    "title": "Classification",
    "section": "",
    "text": "Classification is assigning predefined labels to data based on its features. It facilitates the recognition of patterns and the generalization of insights from data. Typical algorithms that incorporate this style of Machine Learning are Logistic Regression, Decision Trees, and Support Vector Machines. One very applicable use in everyone’s daily life is the spam filter. Data is either classified as spam or ham, algorithms can be tuned with datasets to assign these classes to unlabeled data."
  },
  {
    "objectID": "posts/post_1/index.html#histogram-here-is-a-histogram-of-the-iris-data.",
    "href": "posts/post_1/index.html#histogram-here-is-a-histogram-of-the-iris-data.",
    "title": "Probabiltiy Theory and Random Variables",
    "section": "Histogram Here is a histogram of the iris data.",
    "text": "Histogram Here is a histogram of the iris data.\n\n\nCreating the Histogram\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\n\n# Load the Iris dataset\niris = load_iris()\nsepal_length_setosa = iris.data[iris.target == 0][:, 0]  # Sepal length of Iris setosa\n\n# Define a function to compute the probability density function (PDF)\ndef compute_pdf(data, num_bins=20):\n    \"\"\"\n    Compute the probability density function (PDF) of a dataset.\n\n    Parameters:\n    - data: Input dataset.\n    - num_bins: Number of bins for the histogram.\n\n    Returns:\n    - bin_edges: Bin edges.\n    - pdf_values: PDF values for each bin.\n    \"\"\"\n    hist, bin_edges = np.histogram(data, bins=num_bins, density=True)\n    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n    pdf_values = hist / np.sum(hist)\n    return bin_centers, pdf_values\n\n# Compute the PDF for sepal length of Iris setosa\nbin_centers, pdf_values = compute_pdf(sepal_length_setosa)\n\n# Plot the PDF\nplt.plot(bin_centers, pdf_values, label='PDF - Sepal Length (Iris setosa)')\nplt.title('Probability Density Function (PDF) of Sepal Length (Iris setosa)')\nplt.xlabel('Sepal Length (cm)')\nplt.ylabel('Probability Density')\nplt.legend()\nplt.show()\n\n\n\n\n\n##KDE/PDF Here is an example of a KDE/PDF plot. This shows the estimated density if the data and model were to be generalized. This is the distribution on a larger dataset and the predictions the model would make on a larger test. A PDF is almost the same except it is a rounded curve using the current data set.\n\n\nCreating the KDE\nfrom sklearn.neighbors import KernelDensity\n# Reshape the data to fit the input requirements of KernelDensity\nsepal_length_setosa = sepal_length_setosa.reshape(-1, 1)\n\n# Fit the Kernel Density Estimation model\nkde = KernelDensity(bandwidth=0.6, kernel='gaussian')\nkde.fit(sepal_length_setosa)\n\n# Generate data points for the plot\nx_values = np.linspace(min(sepal_length_setosa), max(sepal_length_setosa), 1000).reshape(-1, 1)\n\n# Score_samples returns the log likelihood of the samples\nlog_density_values = kde.score_samples(x_values)\n\n# Plot the Kernel Density Estimation\nplt.plot(x_values, np.exp(log_density_values), label='Kernel Density Estimation (KDE)')\nplt.title('Kernel Density Estimation (KDE) of Sepal Length (Iris setosa)')\nplt.xlabel('Sepal Length (cm)')\nplt.ylabel('Probability Density')\nplt.legend()\nplt.show()\n\n\n\n\n\nThese are another example of the same plots (Histogram and KDE/PDF) using a different dataset.\n##Histogram\n\n\nCreating the Histogram\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the tips dataset from Seaborn\ntips = sns.load_dataset('tips')\ntotal_bill_data = tips['total_bill']\n\n# Create a histogram to estimate the probability density function (PDF)\nplt.hist(total_bill_data, bins=30, density=True, alpha=0.7, color='blue', label='Histogram')\n\n# Plot formatting\nplt.title('Probability Density Function (PDF) of Total Bill Amount')\nplt.xlabel('Total Bill Amount ($)')\nplt.ylabel('Probability Density')\nplt.legend()\nplt.show()\n\n\n\n\n\n##PDF/KDE You can clearly see that in this plot compared to the previous, the curve is smoothed. This is to generalize the data and not overfit on the training set, it will get more realistic predictions on future data.\n\n\nCreating the PDF\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the tips dataset from Seaborn\ntips = sns.load_dataset('tips')\ntotal_bill_data = tips['total_bill']\n\n# Create a Kernel Density Estimation (KDE) plot for the PDF\nsns.kdeplot(total_bill_data, fill=True, color='blue', label='Probability Density Function (PDF)')\n\n# Plot formatting\nplt.title('Probability Density Function (PDF) of Total Bill Amount')\nplt.xlabel('Total Bill Amount ($)')\nplt.ylabel('Probability Density')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/post_1/index.html#histogram",
    "href": "posts/post_1/index.html#histogram",
    "title": "Probabiltiy Theory and Random Variables",
    "section": "Histogram",
    "text": "Histogram\nHere is a histogram of the iris data.\n\n\nCreating the Histogram\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\n\n# Load the Iris dataset\niris = load_iris()\nsepal_length_setosa = iris.data[iris.target == 0][:, 0]  # Sepal length of Iris setosa\n\n# Define a function to compute the probability density function (PDF)\ndef compute_pdf(data, num_bins=20):\n    \"\"\"\n    Compute the probability density function (PDF) of a dataset.\n\n    Parameters:\n    - data: Input dataset.\n    - num_bins: Number of bins for the histogram.\n\n    Returns:\n    - bin_edges: Bin edges.\n    - pdf_values: PDF values for each bin.\n    \"\"\"\n    hist, bin_edges = np.histogram(data, bins=num_bins, density=True)\n    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n    pdf_values = hist / np.sum(hist)\n    return bin_centers, pdf_values\n\n# Compute the PDF for sepal length of Iris setosa\nbin_centers, pdf_values = compute_pdf(sepal_length_setosa)\n\n# Plot the PDF\nplt.plot(bin_centers, pdf_values, label='PDF - Sepal Length (Iris setosa)')\nplt.title('Probability Density Function (PDF) of Sepal Length (Iris setosa)')\nplt.xlabel('Sepal Length (cm)')\nplt.ylabel('Probability Density')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/post_1/index.html#kdepdf",
    "href": "posts/post_1/index.html#kdepdf",
    "title": "Probabiltiy Theory and Random Variables",
    "section": "KDE/PDF",
    "text": "KDE/PDF\nHere is an example of a KDE/PDF plot. This shows the estimated density if the data and model were to be generalized. This is the distribution on a larger dataset and the predictions the model would make on a larger test. A PDF is almost the same except it is a rounded curve using the current data set.\n\n\nCreating the KDE\nfrom sklearn.neighbors import KernelDensity\n# Reshape the data to fit the input requirements of KernelDensity\nsepal_length_setosa = sepal_length_setosa.reshape(-1, 1)\n\n# Fit the Kernel Density Estimation model\nkde = KernelDensity(bandwidth=0.6, kernel='gaussian')\nkde.fit(sepal_length_setosa)\n\n# Generate data points for the plot\nx_values = np.linspace(min(sepal_length_setosa), max(sepal_length_setosa), 1000).reshape(-1, 1)\n\n# Score_samples returns the log likelihood of the samples\nlog_density_values = kde.score_samples(x_values)\n\n# Plot the Kernel Density Estimation\nplt.plot(x_values, np.exp(log_density_values), label='Kernel Density Estimation (KDE)')\nplt.title('Kernel Density Estimation (KDE) of Sepal Length (Iris setosa)')\nplt.xlabel('Sepal Length (cm)')\nplt.ylabel('Probability Density')\nplt.legend()\nplt.show()\n\n\n\n\n\nThese are another example of the same plots (Histogram and KDE/PDF) using a different dataset."
  },
  {
    "objectID": "posts/post_1/index.html#histogram-1",
    "href": "posts/post_1/index.html#histogram-1",
    "title": "Probabiltiy Theory and Random Variables",
    "section": "Histogram",
    "text": "Histogram\n\n\nCreating the Histogram\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the tips dataset from Seaborn\ntips = sns.load_dataset('tips')\ntotal_bill_data = tips['total_bill']\n\n# Create a histogram to estimate the probability density function (PDF)\nplt.hist(total_bill_data, bins=30, density=True, alpha=0.7, color='skyblue', edgecolor = \"black\", label='Histogram')\n\n# Plot formatting\nplt.title('Probability Density Function (PDF) of Total Bill Amount')\nplt.xlabel('Total Bill Amount ($)')\nplt.ylabel('Probability Density')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/post_1/index.html#pdfkde",
    "href": "posts/post_1/index.html#pdfkde",
    "title": "Probabiltiy Theory and Random Variables",
    "section": "PDF/KDE",
    "text": "PDF/KDE\nYou can clearly see that in this plot compared to the previous, the curve is smoothed. This is to generalize the data and not overfit on the training set, it will get more realistic predictions on future data.\n\n\nCreating the PDF\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the tips dataset from Seaborn\ntips = sns.load_dataset('tips')\ntotal_bill_data = tips['total_bill']\n\n# Create a Kernel Density Estimation (KDE) plot for the PDF\nsns.kdeplot(total_bill_data, fill=True, color='blue', label='Probability Density Function (PDF)')\n\n# Plot formatting\nplt.title('Probability Density Function (PDF) of Total Bill Amount')\nplt.xlabel('Total Bill Amount ($)')\nplt.ylabel('Probability Density')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/post_2/index.html#db-scan",
    "href": "posts/post_2/index.html#db-scan",
    "title": "Clustering",
    "section": "DB Scan",
    "text": "DB Scan\nHere is an instance of DB Scan doing clustering:\n\n\nDB Scan perform and label\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_moons\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nX, y = make_moons(n_samples=500, noise=0.05, random_state=40)\ndbscan = DBSCAN(eps=0.05, min_samples=5)\ndbscan.fit(X)\n\ndef plot_dbscan(dbscan, X, size, show_xlabels=True, show_ylabels=True):\n    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n    core_mask[dbscan.core_sample_indices_] = True\n    anomalies_mask = dbscan.labels_ == -1\n    non_core_mask = ~(core_mask | anomalies_mask)\n\n    cores = dbscan.components_\n    anomalies = X[anomalies_mask]\n    non_cores = X[non_core_mask]\n    \n    plt.scatter(cores[:, 0], cores[:, 1],\n                c=dbscan.labels_[core_mask], marker='o', s=size, cmap=\"Paired\")\n    plt.scatter(cores[:, 0], cores[:, 1], marker='*', s=20,\n                c=dbscan.labels_[core_mask])\n    plt.scatter(anomalies[:, 0], anomalies[:, 1],\n                c=\"r\", marker=\"x\", s=100)\n    plt.scatter(non_cores[:, 0], non_cores[:, 1],\n                c=dbscan.labels_[non_core_mask], marker=\".\")\n    if show_xlabels:\n        plt.xlabel(\"$x_1$\")\n    else:\n        plt.tick_params(labelbottom=False)\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", rotation=0)\n    else:\n        plt.tick_params(labelleft=False)\n    plt.title(f\"eps={dbscan.eps:.2f}, min_samples={dbscan.min_samples}\")\n    plt.grid()\n    plt.gca().set_axisbelow(True)\n\ndbscan2 = DBSCAN(eps=0.1)\ndbscan2.fit(X)\n\nplt.figure(figsize=(9, 3.2))\n\nplt.subplot(121)\nplot_dbscan(dbscan, X, size=100)\n\nplt.subplot(122)\nplot_dbscan(dbscan2, X, size=600, show_ylabels=False)\n\nplt.show()\n\n\n\n\n\nTaking these clusters the user can then put meaning to them and see if it is properly distributed. As you can see the DB Scan first has too small an epsilon and does not properly identify the two meaningful clusters."
  },
  {
    "objectID": "posts/post_2/index.html#db-scan-with-new-data",
    "href": "posts/post_2/index.html#db-scan-with-new-data",
    "title": "Clustering",
    "section": "DB Scan with new data",
    "text": "DB Scan with new data\nDB Scan makes its clusters based on distance between points so it does not require a preset number of clusters to look for.\n\n\nCode for the DB Scan\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_blobs\n\n# Generate sample data with blobs\ndata, _ = make_blobs(n_samples=300, centers=4, random_state=42, cluster_std=1.0)\n\n# Apply DBSCAN\ndbscan = DBSCAN(eps=0.5, min_samples=5)\nlabels = dbscan.fit_predict(data)\n\n# Plot the data points and cluster assignments\nplt.figure(figsize=(8, 6))\n\nunique_labels = set(labels)\ncolors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n\nfor label, color in zip(unique_labels, colors):\n    if label == -1:\n        color = [0, 0, 0, 1]  # Black color for noise points\n\n    class_member_mask = (labels == label)\n    xy = data[class_member_mask]\n    plt.scatter(xy[:, 0], xy[:, 1], c=[color], edgecolors='k', s=50, linewidth=0.5, label=f'Cluster {label}')\n\nplt.title('DBSCAN Clustering')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/post_2/index.html#k-means-clustering",
    "href": "posts/post_2/index.html#k-means-clustering",
    "title": "Clustering",
    "section": "K-Means Clustering",
    "text": "K-Means Clustering\nHere is an example of K-Means clustering with the iris dataset.\n\n\nCode for the K-Means Clustering\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import load_iris\nfrom sklearn.decomposition import PCA\n\n# Load the Iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Apply K-means clustering\nkmeans = KMeans(n_clusters=3, random_state=42)\nkmeans.fit(X)\n\n# Get cluster assignments and centroids\nlabels = kmeans.labels_\ncentroids = kmeans.cluster_centers_\n\n# Reduce data dimensionality for visualization (using PCA)\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\n# Scatter plot of the data points colored by cluster assignment\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', edgecolor='k', s=50, alpha=0.7)\n\n# Plot formatting\nplt.title('K-means Clustering of Iris Dataset')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.show()"
  },
  {
    "objectID": "posts/post_2/index.html#agglomerative-hierarchical-clustering",
    "href": "posts/post_2/index.html#agglomerative-hierarchical-clustering",
    "title": "Clustering",
    "section": "Agglomerative Hierarchical Clustering",
    "text": "Agglomerative Hierarchical Clustering\nThis final example uses Agglomerative Hierarchical Clustering. This form of clustering treats each data point as their own cluster. It gradually joins clusters that are within close proximity, reducing the amount of clusters present. It only stops once there are not any clusters within a certain distance of another.\n\n\nCode for the Agglomerative Hierarchical Clustering\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import AgglomerativeClustering\n\n# Generate synthetic data using make_blobs\nX, y = make_blobs(n_samples=300, centers=3, random_state=39)\n\n# Apply Agglomerative Hierarchical Clustering\nagglomerative = AgglomerativeClustering(n_clusters=3)\nlabels = agglomerative.fit_predict(X)\n\n# Scatter plot of the data points colored by cluster assignment\nplt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', edgecolor='k', s=50, alpha=0.7)\n\n# Plot formatting\nplt.title('Agglomerative Hierarchical Clustering with make_blobs Data')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.show()\n\n\n\n\n\nCode based on the textbook (Hands-on Machine Learning https://www.oreilly.com/library/view/hands-on-machine-learning/9781098125967/)"
  },
  {
    "objectID": "posts/post_4/index.html#cmd-plot",
    "href": "posts/post_4/index.html#cmd-plot",
    "title": "Classification",
    "section": "CMD Plot",
    "text": "CMD Plot\nHere is a Confusion Matrix Display (CMD) Plot. It shows the prediction of the model in comparison with the true label. (0,0) are instances where the model predicts false and the true label is also false. (1,1) are the instances where the model predicts true and the true label is also true.\n\n\nCode for the CMD\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X,\n    y,\n    test_size=10000,\n    random_state=42,\n)\ny_train_5 = y_train == '5'  # True for all 5s, False for all other digits\ny_test_5 = y_test == '5'\n\nsgd_clf = SGDClassifier(random_state=42)\nsgd_clf.fit(X_train, y_train_5)\ny_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=2)\ncm = confusion_matrix(y_train_5, y_train_pred)\ncmd = ConfusionMatrixDisplay(cm)\ncmd.plot(cmap='summer')\nplt.show()"
  },
  {
    "objectID": "posts/post_4/index.html#precision-recall-curve",
    "href": "posts/post_4/index.html#precision-recall-curve",
    "title": "Classification",
    "section": "Precision Recall Curve",
    "text": "Precision Recall Curve\nThis graph shows the precision recall curve which is a similar plot to the ROC but is used when there is more of a class imbalance. ROC curves are used when there is an equal distribution of classes in the dataset. It is used to show the performance of the model on test datasets. Precision is the proportion correct out of the amount predicted for that label. Recall is the amount predicted for that label out of the total correct predictions. With a PR Curve the part of the plot that matters is the top right as you want to balance precision and recall and have the best overall performance of the model.\n\n\nCode for Precision Recall Curve Plot\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import PrecisionRecallDisplay, PredictionErrorDisplay\nfrom sklearn.metrics import precision_recall_curve\n\n\nsgd_clf = SGDClassifier(random_state=42)\nsgd_clf.fit(X_train, y_train_5)\ny_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method=\"decision_function\")\nprecisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\nPrecisionRecallDisplay(precisions, recalls).plot();"
  },
  {
    "objectID": "posts/post_4/index.html#roc-curve",
    "href": "posts/post_4/index.html#roc-curve",
    "title": "Classification",
    "section": "ROC Curve",
    "text": "ROC Curve\nThis is a representation of a ROC Curve. It is a plot of the recall, as explained before, and the false positive rate (amount of false positives divided by the false positives and true negatives).\n\n\nCode for the ROC Curve\nfrom sklearn.metrics import roc_curve\nimport matplotlib.patches as patches  # extra code – for the curved arrow\n\nidx_for_90_precision = (precisions &gt;= 0.90).argmax()\nthreshold_for_90_precision = thresholds[idx_for_90_precision]\n\nfpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\n#| echo: true\nidx_for_threshold_at_90 = (thresholds &lt;= threshold_for_90_precision).argmax()\ntpr_90, fpr_90 = tpr[idx_for_threshold_at_90], fpr[idx_for_threshold_at_90]\n\nplt.figure(figsize=(6, 5))  # extra code – not needed, just formatting\nplt.plot(fpr, tpr, linewidth=2, label=\"ROC curve\")\nplt.plot([0, 1], [0, 1], 'k:', label=\"Random classifier's ROC curve\")\nplt.plot([fpr_90], [tpr_90], \"ko\", label=\"Threshold for 90% precision\")\n\n# extra code – just beautifies and saves Figure 3–7\nplt.gca().add_patch(patches.FancyArrowPatch(\n    (0.20, 0.89), (0.07, 0.70),\n    connectionstyle=\"arc3,rad=.4\",\n    arrowstyle=\"Simple, tail_width=1.5, head_width=8, head_length=10\",\n    color=\"#444444\"))\nplt.text(0.12, 0.71, \"Higher\\nThreshold\", color=\"#333333\")\nplt.xlabel('False Positive Rate (Fall-Out, FPR)')\nplt.ylabel('True Positive Rate (Recall)')\nplt.grid()\nplt.axis([0, 1, 0, 1])\nplt.legend(loc=\"lower right\", fontsize=13)\nsave_fig(\"roc_curve_plot\")\n\nplt.show()\n\n\n\n\n\nCode is from the textbook (Hands-on Machine Learning https://www.oreilly.com/library/view/hands-on-machine-learning/9781098125967/)"
  },
  {
    "objectID": "posts/post_5/index.html#db-scan-with-outliers",
    "href": "posts/post_5/index.html#db-scan-with-outliers",
    "title": "Anomaly and Outlier Detection",
    "section": "DB Scan with Outliers",
    "text": "DB Scan with Outliers\nThis is a DB Scan plot using purposefully created blobs of data classified into clusters. The outliers are marked with a red “X”. This is a good example of the outline detection being performed automatically. They are all placed into their own class. DB Scan automatically flags outliers if they are not close enough in proximity to other data points.\n\n\nCode for the DB Scan\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_blobs\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Generate sample data with blobs\ndata, _ = make_blobs(n_samples=300, centers=4, random_state=40, cluster_std=1.0)\n\n# Apply DBSCAN\ndbscan = DBSCAN(eps=0.6, min_samples=5)\nlabels = dbscan.fit_predict(data)\n\n# Plot the data points and cluster assignments\nplt.figure(figsize=(8, 6))\n\nunique_labels = set(labels)\ncolors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n\nfor label, color in zip(unique_labels, colors):\n    if label == -1:\n        color = [0, 0, 0, 1]  # Black color for noise points\n\n    class_member_mask = (labels == label)\n    xy = data[class_member_mask]\n    plt.scatter(xy[:, 0], xy[:, 1], c=[color], edgecolors='k', s=50, linewidth=0.5, label=f'Cluster {label}')\n    \nanomalies_mask = dbscan.labels_ == -1\nanomalies = data[anomalies_mask]\nplt.scatter(anomalies[:, 0], anomalies[:, 1],\n                c=\"r\", marker=\"x\", s=100)\n\nplt.title('DBSCAN Clustering')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/post_5/index.html#gmm-with-outliers",
    "href": "posts/post_5/index.html#gmm-with-outliers",
    "title": "Anomaly and Outlier Detection",
    "section": "GMM with Outliers",
    "text": "GMM with Outliers\nHere is a plot of the same data being clustered by a Gaussian Mixture Model(GMM). It can also identify anomalies/outliers. In this example, they are identified based on their score exceeding a certain value. They are scored from their distance from the center. A Gaussian Mixture Model involves K-Means clustering with some generalization.\n\n\nCode for the GMM\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.mixture import GaussianMixture\n\n# Generate synthetic data using make_blobs\nX, _ = make_blobs(n_samples=300, centers=4, random_state=40, cluster_std=1.0)\n\n# Fit a Gaussian Mixture Model (GMM) to the data\ngmm = GaussianMixture(n_components=4)\ngmm.fit(X)\n\n# Predict cluster labels for each data point\nlabels = gmm.predict(X)\n\n# Calculate the Mahalanobis distance for each point\nmahalanobis_distance = gmm.score_samples(X)\n\n# Set a threshold to identify outliers\noutlier_threshold = -5  # Adjust as needed\n\n# Identify outliers based on the Mahalanobis distance\noutliers = np.where(mahalanobis_distance &lt; outlier_threshold)[0]\n\n# Generate data points for the plot\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\nZ = -gmm.score_samples(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\n# Plot the Gaussian Mixture Model with cluster labels and outliers\nplt.contour(xx, yy, Z, levels=[0.5], linewidths=2, colors='black', label='Decision Boundary')\nscatter = plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', label='Data Points')\nplt.scatter(X[outliers, 0], X[outliers, 1], color='red', marker='x', s=100, label='Outliers')\n\n# Add legend with cluster labels\nlegend_labels = ['Cluster 0', 'Cluster 1', 'Cluster 2', 'Cluster 3', 'Outliers']\nplt.legend(handles=scatter.legend_elements()[0], title='Clusters', labels=legend_labels)\n\n# Plot formatting\nplt.title('Gaussian Mixture Model (GMM) with random Data and Outliers')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.show()"
  },
  {
    "objectID": "posts/post_3/index.html#linear-prediction",
    "href": "posts/post_3/index.html#linear-prediction",
    "title": "Linear and Non-Linear Regression",
    "section": "",
    "text": "Here you can see a linear prediction based on random data with a linear trend. The model finds the most accurate linear expression based on the given data and graphs its results. Since we created a linear relationship it will have a high accuracy. In other data sets there will not always be a linear relationship to find, which will require non linear regression.\n\n\n\n\n\nWhen predicting trends the algorithm can have high-bias and underfit the data. This leads to it being too simple and not notice more applicable functions. The algorithm can also get high-variance or overfitting in which the model is too trained to the training data that it does not predict accurately on test data.\nRidge and Lasso Regression are regularization techniques to prevent over fitting and improve the performance of linear regression models by adding a penalty term.\nRidge Regression aims to minimize the sum of squared differences between the observed and predicted values. Lasso Regression uses the absolute value of the coefficients and uses a penalty term."
  },
  {
    "objectID": "posts/post_3/index.html#non-linear-example",
    "href": "posts/post_3/index.html#non-linear-example",
    "title": "Linear and Non-Linear Regression",
    "section": "Non Linear Example",
    "text": "Non Linear Example\nThis plot is using the same dataset from before."
  },
  {
    "objectID": "posts/post_3/index.html#linear-vs-non-linear-accuracy",
    "href": "posts/post_3/index.html#linear-vs-non-linear-accuracy",
    "title": "Linear and Non-Linear Regression",
    "section": "Linear vs Non Linear Accuracy",
    "text": "Linear vs Non Linear Accuracy\nHere are the r2 score of the two plots from above:\n\n\nLinear Regression r² Score: 96%\nRandom Forest Regression r² Score: 99%\n\n\nThis is a plot of the accuracy between SVM (Linear) and Random Forest (Non Linear). Two very different supervised learning models. This plot shows their performance in predicting trends on data.\n\n\nCode\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\nX_train = preprocess_pipeline.fit_transform(train_data)\ny_train = train_data[\"Survived\"]\nforest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\nforest_clf.fit(X_train, y_train)\nX_test = preprocess_pipeline.transform(test_data)\ny_pred = forest_clf.predict(X_test)\nforest_scores = cross_val_score(forest_clf, X_train, y_train, cv=10)\nforest_scores.mean()\n\nfrom sklearn.svm import SVC\n\nsvm_clf = SVC(gamma=\"auto\")\nsvm_scores = cross_val_score(svm_clf, X_train, y_train, cv=10)\nsvm_scores.mean()\n\n\nplt.figure(figsize=(8, 4))\nplt.plot([1]*10, svm_scores, \".\")\nplt.plot([2]*10, forest_scores, \".\")\nplt.boxplot([svm_scores, forest_scores], labels=(\"SVM\", \"Random Forest\"))\nplt.ylabel(\"Accuracy\")\nplt.show()"
  }
]